
@misc{berners-lee_linked_2006,
	title = {Linked {Data}},
	url = {https://www.w3.org/DesignIssues/LinkedData.html},
	author = {Berners-Lee, Tim},
	month = jul,
	year = {2006},
	file = {Berners-Lee - 2006 - Linked Data.pdf:C\:\\Users\\ant87282\\Zotero\\storage\\7QGSJJXE\\Berners-Lee - 2006 - Linked Data.pdf:application/pdf},
}

@incollection{lindgren_linked_2019,
	address = {Cham},
	title = {Linked {Data} in the {European} {Data} {Portal}: {A} {Comprehensive} {Platform} for {Applying} {DCAT}-{AP}},
	volume = {11685},
	isbn = {978-3-030-27324-8 978-3-030-27325-5},
	shorttitle = {Linked {Data} in the {European} {Data} {Portal}},
	url = {https://link.springer.com/10.1007/978-3-030-27325-5_15},
	abstract = {Abstract
            
              The European Data Portal (EDP) is a central access point for metadata of Open Data published by public authorities in Europe and acquires data from more than 70 national data providers. The platform is a starting point in adopting the Linked Data specification DCAT-AP, aiming to increase interoperability and accessibility of Open Data. In this paper, we present the design of the central data management components of the platform, responsible for metadata storage, data harvesting and quality assessment. The core component is based on CKAN, which is extended by the support for native Linked Data replication to a triplestore to ensure legacy compatibility and the support for DCAT-AP. Regular data harvesting and the creation of detailed quality reports are performed by custom components adressing the requirements of DCAT-AP. The EDP is well on track to become the core platform for European Open Data and fostered the acceptance of DCAT-AP. Our platform is available here:
              https://www.europeandataportal.eu
              .},
	language = {en},
	urldate = {2024-04-09},
	booktitle = {Electronic {Government}},
	publisher = {Springer International Publishing},
	author = {Kirstein, Fabian and Dittwald, Benjamin and Dutkowski, Simon and Glikman, Yury and Schimmler, Sonja and Hauswirth, Manfred},
	editor = {Lindgren, Ida and Janssen, Marijn and Lee, Habin and Polini, Andrea and Rodríguez Bolívar, Manuel Pedro and Scholl, Hans Jochen and Tambouris, Efthimios},
	year = {2019},
	doi = {10.1007/978-3-030-27325-5_15},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Data quality, Linked Data},
	pages = {192--204},
	file = {Volltext:C\:\\Users\\ant87282\\Zotero\\storage\\GDLCVBTD\\Kirstein et al. - 2019 - Linked Data in the European Data Portal A Compreh.pdf:application/pdf},
}

@book{bruns_leitfaden_2019,
	title = {Leitfaden für qualitativ hochwertige {Daten} und {Metadaten}},
	shorttitle = {{NQDM}-{Leitfaden}},
	url = {https://nqdm-projekt.de/de/downloads/leitfaden},
	abstract = {Dieser Leitfaden bietet praktische Hilfestellungen und Empfehlungen zur Erreichung einer hohen Datenund Metadatenqualität. Die enthaltenen Empfehlungen können grundsätzlich auf jegliche Art von Daten
angewendet werden, unabhängig von Zugänglichkeit, Herkunft und dem sektoralen Bezug. Besonders ist
der Leitfaden für Datenbereitsteller aus der öffentlichen Verwaltung empfehlenswert, die ihre Daten als
Open Data veröffentlichen.
Im Leitfaden werden unterschiedliche Qualitätsdimensionen, Datenstrukturtypen und Bewertungsschemata
für die Qualität von Daten und Metadaten aufgezeigt. Gängige maschinenlesbare und offene Daten- und
Schnittstellenformate werden vorgestellt und anhand anschaulicher Beispiele wird aufgezeigt, wie eine
hohe Datenqualität erreicht werden kann.
Der Leitfaden wurde im Rahmen des Projektes NQDM – Normentwurf für qualitativ hochwertige Daten und
Metadaten – von Fraunhofer FOKUS im Zeitraum von September 2017 bis August 2019 erstellt.
Weiterführende Informationen zu dem Projekt können unter https://www.nqdm-projekt.de/ eingesehen
werden.},
	language = {Deutsch},
	publisher = {Fraunhofer-Institut für Offene Kommunikationssysteme FOKUS},
	author = {Bruns, Lina and Dittwald, Benjamin and Meiners, Fritz},
	year = {2019},
	keywords = {Datenqualität, Open Data},
	file = {_.pdf:C\:\\Users\\ant87282\\Zotero\\storage\\ZQAYCKSW\\_.pdf:application/pdf},
}

@article{pipino_data_2002,
	title = {Data {Quality} {Assessment}},
	volume = {45},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/505248.506010},
	doi = {10.1145/505248.506010},
	abstract = {How good is a company's data quality? Answering this question requires usable data quality metrics. Currently, most data quality measures are developed on an ad hoc basis to solve specific problems [6, 8], and fundamental principles necessary for developing usable metrics in practice are lacking. In this article, we describe principles that can help organizations develop usable data quality metrics.},
	language = {en},
	number = {4},
	urldate = {2024-04-22},
	journal = {Communications of the ACM},
	author = {Pipino, Leo L. and Lee, Yang W. and Wang, Richard Y.},
	month = apr,
	year = {2002},
	keywords = {Datenqualität, Open Data},
	pages = {211--218},
	file = {Full Text PDF:C\:\\Users\\ant87282\\Zotero\\storage\\ZQIL2LVE\\Pipino et al. - 2002 - Data quality assessment.pdf:application/pdf},
}

@inproceedings{vaddepalli_taxonomy_2023,
	address = {Singapore},
	title = {Taxonomy of {Data} {Quality} {Metrics} in {Digital} {Citizen} {Science}},
	isbn = {978-981-19766-0-5},
	doi = {10.1007/978-981-19-7660-5_34},
	abstract = {Data quality is key in the success of a citizen science project. Valid datasets serve as evidence for scientific research. Numerous projects have highlighted the ways in which participatory data collection can cause data quality issues due to human day-to-day practices and biases. Also, these projects have used and reported a myriad of techniques to improve data quality in different contexts. Yet, there is a lack of systematic analyses of these experiences to guide the design and of digital citizen science projects. We mapped 35 data quality issues of 16 digital citizen science projects and proposed a taxonomy with 64 mechanisms to address data quality issues before, during and after the data collection in digital citizen science projects. This taxonomy is built upon the analysis of literature reports (N = 144), two urban experiments (participants = 280), and expert interviews (N = 11). Thus, we contribute to advance the development of systematic methods to improve the data quality in digital citizen science projects.},
	language = {en},
	booktitle = {Intelligent {Sustainable} {Systems}},
	publisher = {Springer Nature},
	author = {Vaddepalli, Krishna and Palacin, Victoria and Porras, Jari and Happonen, Ari},
	editor = {Nagar, Atulya K. and Singh Jat, Dharm and Mishra, Durgesh Kumar and Joshi, Amit},
	year = {2023},
	keywords = {Datenqualität, Open Data},
	pages = {391--410},
	file = {Full Text PDF:C\:\\Users\\ant87282\\Zotero\\storage\\5N7VHBZU\\Vaddepalli et al. - 2023 - Taxonomy of Data Quality Metrics in Digital Citize.pdf:application/pdf},
}

@article{vetro_open_2016,
	title = {Open data quality measurement framework: {Definition} and application to {Open} {Government} {Data}},
	volume = {33},
	issn = {0740-624X},
	shorttitle = {Open data quality measurement framework},
	url = {https://www.sciencedirect.com/science/article/pii/S0740624X16300132},
	doi = {10.1016/j.giq.2016.02.001},
	abstract = {The diffusion of Open Government Data (OGD) in recent years kept a very fast pace. However, evidence from practitioners shows that disclosing data without proper quality control may jeopardize dataset reuse and negatively affect civic participation. Current approaches to the problem in literature lack a comprehensive theoretical framework. Moreover, most of the evaluations concentrate on open data platforms, rather than on datasets. In this work, we address these two limitations and set up a framework of indicators to measure the quality of Open Government Data on a series of data quality dimensions at most granular level of measurement. We validated the evaluation framework by applying it to compare two cases of Italian OGD datasets: an internationally recognized good example of OGD, with centralized disclosure and extensive data quality controls, and samples of OGD from decentralized data disclosure (municipality level), with no possibility of extensive quality controls as in the former case, hence with supposed lower quality. Starting from measurements based on the quality framework, we were able to verify the difference in quality: the measures showed a few common acquired good practices and weaknesses, and a set of discriminating factors that pertain to the type of datasets and the overall approach. On the basis of this evaluation, we also provided technical and policy guidelines to overcome the weaknesses observed in the decentralized release policy, addressing specific quality aspects.},
	number = {2},
	urldate = {2024-04-22},
	journal = {Government Information Quarterly},
	author = {Vetrò, Antonio and Canova, Lorenzo and Torchiano, Marco and Minotas, Camilo Orozco and Iemma, Raimondo and Morando, Federico},
	month = apr,
	year = {2016},
	keywords = {Datenqualität, Open Data},
	pages = {325--337},
	file = {Full Text:C\:\\Users\\ant87282\\Zotero\\storage\\95AYPPPF\\Vetrò et al. - 2016 - Open data quality measurement framework Definitio.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\ant87282\\Zotero\\storage\\3ICYTV2A\\S0740624X16300132.html:text/html},
}

@article{behkamal_metrics-driven_2014,
	title = {A metrics-driven approach for quality assessment of linked open data},
	volume = {9},
	issn = {0718-1876},
	doi = {10.4067/S0718-18762014000200006},
	abstract = {The main objective of the Web of Data paradigm is to crystallize knowledge through the interlinking of already existing but dispersed data. The usefulness of the developed knowledge depends strongly on the quality of the published data. Researchers have observed many deficiencies with regard to the quality of Linked Open Data. The first step towards improving the quality of data released as a part of the Linked Open Data Cloud is to develop tools for measuring the quality of such data. To this end, the main objective of this paper is to propose and validate a set of metrics for evaluating the inherent quality characteristics of a dataset before it is released to the Linked Open Data Cloud. These inherent characteristics are semantic accuracy, syntactic accuracy, uniqueness, completeness and consistency. We follow the Goal-Question-Metric approach to propose various metrics for each of these five quality characteristics. We provide both theoretical validation and empirical observation of the behavior of the proposed metrics in this paper. The proposed set of metrics establishes a starting point for a systematic inherent quality analysis of open datasets. © 2014 Universidad de Talca - Chile.},
	language = {English},
	number = {2},
	journal = {Journal of Theoretical and Applied Electronic Commerce Research},
	author = {Behkamal, B. and Kahani, M. and Bagheri, E. and Jeremic, Z.},
	year = {2014},
	keywords = {Linked Data, Datenqualität, Open Data},
	pages = {64--79},
	annote = {Cited By :48},
	file = {Full Text:C\:\\Users\\ant87282\\Zotero\\storage\\DEHKQJ83\\Behkamal et al. - 2014 - A metrics-driven approach for quality assessment o.pdf:application/pdf;Snapshot:C\:\\Users\\ant87282\\Zotero\\storage\\CBQHIFWT\\display.html:text/html},
}

@article{neumaier_automated_2016,
	title = {Automated {Quality} {Assessment} of {Metadata} across {Open} {Data} {Portals}},
	volume = {8},
	issn = {1936-1955},
	url = {https://dl.acm.org/doi/10.1145/2964909},
	doi = {10.1145/2964909},
	abstract = {The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.},
	number = {1},
	urldate = {2024-04-23},
	journal = {Journal of Data and Information Quality},
	author = {Neumaier, Sebastian and Umbrich, Jürgen and Polleres, Axel},
	year = {2016},
	pages = {2:1--2:29},
	file = {Full Text PDF:C\:\\Users\\ant87282\\Zotero\\storage\\A7M4FUT8\\Neumaier et al. - 2016 - Automated Quality Assessment of Metadata across Op.pdf:application/pdf},
}

@book{przyborski_qualitative_2014,
	address = {München},
	edition = {4., erweiterte Auflage},
	series = {Lehr- und {Handbücher} der {Soziologie}},
	title = {Qualitative {Sozialforschung}: {Ein} {Arbeitsbuch}},
	isbn = {978-3-486-70892-9},
	shorttitle = {Qualitative {Sozialforschung}},
	language = {de},
	publisher = {Oldenbourg Verlag},
	author = {Przyborski, Aglaja and Wohlrab-Sahr, Monika},
	year = {2014},
	keywords = {Datenqualität, Empirische Sozialdaten},
	file = {Przyborski and Wohlrab-Sahr - 2014 - Qualitative Sozialforschung Ein Arbeitsbuch.pdf:C\:\\Users\\ant87282\\Zotero\\storage\\2SLNSYKR\\Przyborski and Wohlrab-Sahr - 2014 - Qualitative Sozialforschung Ein Arbeitsbuch.pdf:application/pdf},
}

@book{hader_empirische_2019,
	address = {Wiesbaden},
	title = {Empirische {Sozialforschung}: {Eine} {Einführung}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-658-26985-2 978-3-658-26986-9},
	shorttitle = {Empirische {Sozialforschung}},
	url = {http://link.springer.com/10.1007/978-3-658-26986-9},
	language = {de},
	urldate = {2024-04-24},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Häder, Michael},
	year = {2019},
	doi = {10.1007/978-3-658-26986-9},
	keywords = {Datenqualität, Empirische Sozialdaten},
	file = {Häder - 2019 - Empirische Sozialforschung Eine Einführung.pdf:C\:\\Users\\ant87282\\Zotero\\storage\\38962SL4\\Häder - 2019 - Empirische Sozialforschung Eine Einführung.pdf:application/pdf},
}

@misc{noauthor_verhaltenskodex_2018,
	title = {Verhaltenskodex für europäische {Statistiken}},
	url = {https://ec.europa.eu/eurostat/documents/4031688/9394019/KS-02-18-142-DE-N.pdf},
	publisher = {Amt für Veröffentlichungen der Europäischen Union},
	year = {2018},
	keywords = {Datenqualität, Statistikdaten},
	file = {2018 - Verhaltenskodex für europäische Statistiken.pdf:C\:\\Users\\ant87282\\Zotero\\storage\\ZW88DNDT\\2018 - Verhaltenskodex für europäische Statistiken.pdf:application/pdf},
}

@article{saidani_qualitatsdimensionen_2023,
	title = {Qualitätsdimensionen maschinellen {Lernens} in der amtlichen {Statistik}},
	volume = {17},
	issn = {1863-8163},
	url = {https://doi.org/10.1007/s11943-023-00329-7},
	doi = {10.1007/s11943-023-00329-7},
	abstract = {Die amtliche Statistik zeichnet sich durch ihren gesetzlich auferlegten Fokus auf die Qualität ihrer Veröffentlichungen aus. Dabei folgt sie den europäischen Qualitätsrahmenwerken, die auf nationaler Ebene in Form von Qualitätshandbüchern konkretisiert und operationalisiert werden, sich jedoch bis dato hinsichtlich Ausgestaltung und Interpretation an den Anforderungen der „klassischen“ Statistikproduktion orientieren. Der zunehmende Einsatz maschineller Lernverfahren (ML) in der amtlichen Statistik muss daher zur Erfüllung des Qualitätsanspruchs durch ein spezifisches, darauf zugeschnittenes Qualitätsrahmenwerk begleitet werden. Das vorliegende Papier leistet einen Beitrag zur Erarbeitung eines solchen Qualitätsrahmenwerks für den Einsatz von ML in der amtlichen Statistik, indem es (1) durch den Vergleich mit bestehenden Qualitätsgrundsätzen des Verhaltenskodex für Europäische Statistiken relevante Qualitätsdimensionen für ML identifiziert und (2) diese unter Berücksichtigung der besonderen methodischen Gegebenheiten von ML ausarbeitet. Dabei (2a) ergänzt es bestehende Vorschläge durch den Aspekt der Robustheit, (2b) stellt Bezug zu den Querschnittsthemen Machine Learning Operations (MLOps) und Fairness her und (2c) schlägt vor, wie die Qualitätssicherung der einzelnen Dimensionen in der Praxis der amtlichen Statistik ausgestaltet werden kann. Diese Arbeit liefert die konzeptionelle Grundlage, um Qualitätsindikatoren für ML-Verfahren formell in die Instrumente des Qualitätsmanagements im Statistischen Verbund zu überführen und damit langfristig den hohen Qualitätsstandard amtlicher Statistik auch bei Nutzung neuer Verfahren zu sichern.},
	language = {de},
	number = {3},
	urldate = {2024-04-24},
	journal = {AStA Wirtschafts- und Sozialstatistisches Archiv},
	author = {Saidani, Younes and Dumpert, Florian and Borgs, Christian and Brand, Alexander and Nickl, Andreas and Rittmann, Alexandra and Rohde, Johannes and Salwiczek, Christian and Storfinger, Nina and Straub, Selina},
	month = dec,
	year = {2023},
	pages = {253--303},
	file = {Full Text PDF:C\:\\Users\\ant87282\\Zotero\\storage\\6XLJRXEW\\Saidani et al. - 2023 - Qualitätsdimensionen maschinellen Lernens in der a.pdf:application/pdf},
}

@misc{noauthor_fundamental_2014,
	title = {Fundamental {Principles} of {National} {Official} {Statistics}},
	url = {https://unstats.un.org/unsd/dnss/gp/fundprinciples.aspx},
	publisher = {United Nations Statistics Divison},
	year = {2014},
	keywords = {Datenqualität, Statistikdaten},
	file = {2014 - Fundamental Principles of National Official Statis.pdf:C\:\\Users\\ant87282\\Zotero\\storage\\YLPHII2Q\\2014 - Fundamental Principles of National Official Statis.pdf:application/pdf},
}

@incollection{zehnder_datenmanipulation_1987,
	address = {Wiesbaden},
	title = {Datenmanipulation},
	isbn = {978-3-322-94122-0},
	url = {https://doi.org/10.1007/978-3-322-94122-0_4},
	abstract = {In einer Datenbank gespeicherte Daten haben für den Benutzer erst dann einen Sinn, wenn sie wieder abgefragt oder in anderer Weise benützt und bearbeitet, allgemein gesagt manipuliert werden können. Schon in der Übersicht (Unterabschnitt 1.5.3) haben wir dabei Abfragen und Mutationen unterschieden.— Abfragen bilden die häufigste Art, Daten zu manipulieren. Es stellt sich in diesem Zusammenhang das bei einer Datenbank nicht-triviale Problem der geeigneten Auswahl einer Teilmenge des Datenbestandes. In Kapitel 4 kommen die logischen Aspekte dieses Auswahlverfahrens zur Sprache, in Abschnitt 5.4 die physischen Zugriffspfade.— Auch bei Mutationen ist der Prozess der Datenauswahl von Bedeutung, da dem System mitgeteilt werden muss, welche Teilmenge der Daten verändert werden soil. Dazu kommen aber noch vielschichtige Probleme der Datenintegrität, denn bei jeder einzelnen Mutation muss man sicherstellen, dass der Inhalt der Datenbank konsistent bleibt. Ihrer Bedeutung wegen werden die Aspekte der Datenintegrität in Kapitel 6 zusammenfassend behandelt.},
	language = {de},
	urldate = {2024-04-26},
	booktitle = {Informationssysteme und {Datenbanken}},
	publisher = {Vieweg+Teubner Verlag},
	author = {Zehnder, Carl August},
	year = {1987},
	doi = {10.1007/978-3-322-94122-0_4},
	keywords = {Datenmanipulation},
	pages = {110--159},
	file = {Zehnder - 1987 - Datenmanipulation.pdf:C\:\\Users\\ant87282\\Zotero\\storage\\V9BYIR46\\Zehnder - 1987 - Datenmanipulation.pdf:application/pdf},
}

@article{wickham_tidy_2014,
	title = {Tidy {Data}},
	volume = {59},
	copyright = {Copyright (c) 2013 Hadley  Wickham},
	issn = {1548-7660},
	url = {https://doi.org/10.18637/jss.v059.i10},
	doi = {10.18637/jss.v059.i10},
	abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
	language = {en},
	urldate = {2024-04-26},
	journal = {Journal of Statistical Software},
	author = {Wickham, Hadley},
	month = sep,
	year = {2014},
	keywords = {Datenmanipulation},
	pages = {1--23},
	file = {Wickham - 2014 - Tidy Data.pdf:C\:\\Users\\ant87282\\Zotero\\storage\\XGLMIIUT\\Wickham - 2014 - Tidy Data.pdf:application/pdf},
}

@book{groves_federal_2017,
	address = {Washington, D.C.},
	title = {Federal {Statistics}, {Multiple} {Data} {Sources}, and {Privacy} {Protection}: {Next} {Steps}},
	isbn = {978-0-309-46537-3},
	shorttitle = {Federal {Statistics}, {Multiple} {Data} {Sources}, and {Privacy} {Protection}},
	url = {https://www.nap.edu/catalog/24893},
	urldate = {2024-04-30},
	publisher = {National Academies Press},
	editor = {Groves, Robert M. and Harris-Kojetin, Brian A.},
	collaborator = {{Panel on Improving Federal Statistics for Policy and Social Science Research Using Multiple Data Sources and State-of-the-Art Estimation Methods} and {Committee on National Statistics} and {Division of Behavioral and Social Sciences and Education} and {National Academies of Sciences, Engineering, and Medicine}},
	month = dec,
	year = {2017},
	doi = {10.17226/24893},
	keywords = {Datenqualität, Qualitiy Framework},
	file = {Full Text:C\:\\Users\\ant87282\\Zotero\\storage\\6BI8SV4D\\Groves and Harris-Kojetin - 2017 - Federal Statistics, Multiple Data Sources, and Pri.pdf:application/pdf},
}
