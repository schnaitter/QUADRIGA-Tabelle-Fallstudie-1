# Datenqualität

**Hier:**Einführung Erläuterung: strukturierte Daten, Fokus Datentyp, Struktur ist nicht gleich Struktur


**Grobziel:**
Nach Absolvierung der QER-Gruppe sind die Lernenden in der Lage, die Datenquellen und die Qualität der Datensätze zu bewerten sowie die Reproduzierbarkeit der Fallstudie zu evaluieren.

**Feinziel:**
Die Lernenden können die Qualität von Datensätzen bewerten.

*Der Forschende lädt sich den zugehörigen Datensatz zum Bildungsberichts 2022 als xlsx Datei herunter (Link) zu den Datensätzen des Bildungsberichts). Als der Forschende sich die Daten genauer anschaut, stellt er fest, dass es sich um aggregierte Daten handelt. Als Quelle für die Rohdaten wird auf das Statistische Bundesamt (Destatis) verwiesen. Hier findet er die Rohdaten im CVS-Format (Link zu Destatis). Wie offen und technisch verwendbar sind diese Datensätze für seine nachfolgenden Analysen? *

**Datenverknüpfung/ Quality Frameworks**
->passt hier nicht so ganz oder?

Im Kontext von administrativen Datenstrukturen ist die Verknüpfbarkeit von Datensätzen von entscheidender Bedeutung (Groves & Harris-Kojetin, 2017, pp. 118–120). Wenn in unterschiedlichen Datensätzen verschiedene Informationen zu Personen gesammelt werden, kann eine effiziente Verknüpfung nur über ein in allen Datensätzen vorhandenes gleiches Variablenmerkmal (z.B. Name, Steuer-ID, Sozialversicherungsnummer) gesetzte werden. Dieses Variablenmerkmal sollte im besten Fall einzigartig sein und eine Person klar identifizieren können. Groves und Harris-Kojetin (2017, p. 118) zeigen die Wichtigkeit dieser Konsistenz am Beispiel der Namensgebung auf: Wenn die Person Dr. Max Tom Mustermann in verschiedenen Datensätzen mal als „Max Mustermann“ oder „Dr. Mustermann“ oder auch „Max Tom Mustermann“ abgespeichert wird, ist eine simple Verknüpfung der Informationen aus den Datensätzen mittels des Namens eine Person nicht möglich. Zudem ist zu beachten, dass sich Namen doppeln können und es sich hier nicht um ein einzigartiges Merkmal handelt. Hier müssten weiter Merkmale (z.B. Adresse, Ausweisnummer) in Kombination verwendet werden, um ein Individuum klar zu identifizieren und dessen Informationen zu aggregieren.

# Qualitätsmerkmale für (Open) Daten und Metadaten

Die inhaltliche und kontextuelle Qualität von Daten und
Metadaten kann anhand von verschiedenen Qualitätsmerkmalen erfasst werden. In
der Fachliteratur variieren die einzelnen Dimensionen je nach Fachgebiet und
Kontext (Behkamal et al., 2014; Bruns et al., 2019; Neumaier et al., 2016;
Vetrò et al., 2016). Die nachfolgenden Qualitätsmerkmale bilden eine
fachübergreifende Basis zur Evaluation der Datenqualität:

- **Fehlerfreiheit:** Die Daten und Metadaten sind fehlerfrei. Somit sind nur korrekte Werte in der Datei vorhanden (Pipino et al., 2002; Vaddepalli et al., 2023).

- **Aktualität:** Die Daten und Metadaten sind auf dem neuesten Stand und werden in regelmäßigen Intervallen überprüft. Das Aktualisierungsintervall ist in den Metadaten mit angegeben. Die Datei enthält eine Versionsnummer, aus welcher erkenntlich wird, auf welchem zeitlichen Stand die Datei ist (Pipino et al., 2002; Vaddepalli et al., 2023).

- **Genauigkeit:** Die Daten und Metadaten sind so präzise wie möglich angegeben. Demnach wird auf Rundungen von Zahlen verzichtet. Die Metadaten enthalten alle relevanten Details zur Datei (Behkamal et al., 2014; Bruns et al., 2019; Vetrò et al., 2016).

- **Konformität:** Die Daten und Metadaten entsprechen den domänenspezifischen Standards. Diese beziehen sich auf Datumsangaben, Zeichenkodierung, etc.. Zudem sind alle Informationen enthalten, welche durch verendetes Vokabular und Betitelung impliziert werden (Behkamal et al., 2014; Vetrò et al., 2016).

- **Konsistenz:** Daten und Metadaten sind widerspruchsfrei. Dies gilt für die Daten selbst und auch in Bezug auf andere Datensätze(Behkamal et al., 2014; Pipino et al., 2002).

- **Vertrauenswürdigkeit:** Der Ursprung der Daten ist kenntlich gemacht. Zudem sollte eine Evaluation bezüglich der Glaubwürdigkeit des Herausgebers erfolgen (Bruns et al., 2019; Pipino et al., 2002).

- **Transparenz:** Veränderungen an den Daten ist für Dritte ersichtlich (beispielsweise durch die Angabe einer Versionsnummer) (Bruns et al., 2019).

- **Verständlichkeit:** Die Daten sind so strukturiert und bezeichnet, dass Außenstehende dieses leicht verstehen können. Es wird nur einfaches Vokabular verwendet, welches kein besonderes Fachwissen voraussetzt (Behkamal et al., 2014; Pipino et al., 2002; Vetrò et al., 2016).

- **Vollständigkeit:** Die Daten sind vollständig (z.B. sind alle Datenfelder befüllt). Falls die Daten unvollständig sind, wird auf die Unvollständigkeit hingewiesen (Behkamal et al., 2014; Pipino et al., 2002; Vetrò et al., 2016).

- **Zugänglichkeit:** Die Daten können auf einfache Weise abgerufen werden. Außerdem besteht eine permanente Verlinkung aller Referenzen und Links (z.B. mittels Verwendung permanenter URIs) (Behkamal et al., 2014; Pipino et al., 2002; Vaddepalli et al., 2023; Vetrò et al., 2016).

# 5-Star-Model

...

# Gütekriterien der empirischen Sozialforschung

Als Gütekriterien werden Qualitätsmerkmale bezeichnet, welche zur Bewertung von Forschungskonzeption und -auswertung herangezogen werden. Die drei klassischen Gütekriterien „Objektivität, Validität und Reliabilität“ setzten hierbei einen weitverbreiteten Qualitätsstandard insbesondere in der empirischen Sozialforschung (Häder, 2019, p. 109; Przyborski & Wohlrab Sahr, 2014, p. 11). Nachfolgend werden diese Kriterien genauer beleuchtet:

**Objektivität:** Die Objektivität einer Untersuchung beinhaltet die Minimierung von äußerlichem Einfluss auf die Untersuchungsteilnehmer*innen. Die teilnehmende Person darf nicht durch die Umgebung, Art der Fragestellung oder einem direkten persönlichem Kontakt innerhalb der Untersuchung zu einem anderen Reaktions- bzw. Antwortverhalten beeinflusst werden. Somit sollten die Ergebnisse unabhängig von der Person sein, welche die Messinstrumente verwendet hat (Häder, 2019, p. 109; Przyborski & Wohlrab-Sahr, 2014, p. 26). 
- Wird die teilnehmende Person manipuliert oder beeinflusst?

**Validität:** Die Validität einer Untersuchung bezeichnet die Übereinstimmung von dem zu untersuchenden Konzept und der tatsächlichen Untersuchung. Damit eine Untersuchung valide ist, müssen die verwendeten Messinstrumente angemessen und adäquat zur Forschungsfrage passen (Häder, 2019, p. 115; Przyborski & Wohlrab-Sahr, 2014, pp. 22, 23). 
- Wird tatsächlich das gemessen, was eigentlich gemessen werden soll?

**Reliabilität:** Die Reliabilität bezeichnet die Zuverlässigkeit einer Methode. Hierunter fällt die Reproduzierbarkeit einer Untersuchung bezüglich der Durchführung als auch der Messergebnisse (Häder, 2019, p. 110; Przyborski & Wohlrab-Sahr, 2014, p. 24). 
- Kommen bei Wiederholung der Untersuchung die gleichen Ergebnisse bei raus?

# Qualitätskriterien von amtlichen Statistikdaten

Im Rahmen des Verhaltenskodex für europäische Statistiken der Europäischen Union werden Qualitätskriterien für die statistische Datenerhebung und -aufbereitung aufgelistet und erläutert (Verhaltenskodex Für Europäische Statistiken, 2018). Dieser Kodex legt die Basis für einen qualitativ hochwertigen Umgang mit statistischen Daten und ist angelehnt an die zehn Fundamental Principles of Official Statistics der Vereinten Nationen (Fundamental Principles of National Official Statistics, 2014). Im nachfolgenden werden die Qualitätskriterien des Verhaltenskodex in die zwei Teilgebiete „Statistische Prozesse“ und „Statistische Produkte“ unterteilt und näher begutachtet:

## Qualitätsmerkmale für Statistische Prozesse

**Solide Methodik:** Um eine qualitativ hochwertige Statistik zu erarbeiten, benötigt es ein wissenschaftlich fundiertes Vorgehen. Dies setzt eine fachliche Expertise voraus, um ein adäquates Vorgehen inklusive der möglichen Messinstrumente bewerten zu können.

**Geeignete statistische Verfahren:** Das gewählte Verfahren muss wissenschaftlich begründet, testsicher und überwachbar sein. Dies gilt für alle statistischen Verfahren von der Datenerhebung bis hin zur Auswertung und Validierung.

**Vermeidung einer übermäßigen Belastung der Auskunftgebenden:** Der Beantwortungsaufwand für die teilnehmenden Personen muss minimal gehalten werden und immer im angemessene Verhältnis zur Untersuchung stehen.

**Wirtschaftlichkeit:** Eine effektive Ressourceneinteilung und ein verhältnismäßiger Untersuchungsaufwand werden angestrebt.

## Qualitätsmerkmale für statistische Produkte

**Relevanz:** Die bereitgestellten Statistiken bieten Mehrwert für die Nutzer*innen.

**Genauigkeit und Zuverlässigkeit:** Die bereitgestellten Statistiken sind fehlerfrei und detailliert.

**Aktualität und Pünktlichkeit:** Die bereitgestellten Statistiken werden regelmäßig aktualisiert und zum angesetzten Zeitpunkt fristgerecht veröffentlicht.

**Kohärenz und Vergleichbarkeit:** Die bereitgestellten Statistiken sind untereinander und im Zeitverlauf konsistent und mit anderen staatlich öffentlichten Statistiken vergleichbar. Hierbei steht die Möglichkeit der Datenkombination aus verschiedenen öffentlichen Institutionen im Vordergrund.

**Zugänglichkeit und Klarheit:** Die bereitgestellten Statistiken werden öffentlich zur Verfügung gestellt und sind leicht auffindbar. Zudem wird transparent und detailreich die Statistik inklusive der Datenerhebung und -auswertung sowie Inhalt zu den beteiligten Parteien angegeben.

# Übung 1 Wimmelbild
...